"""
This file is derived from https://github.com/shelhamer/ourl/envs.py
Originally written by Evan Shelhamer and modified by Deepak Pathak
"""
from collections import deque
import numpy as np
from PIL import Image
import gym


class OneHotWrapper(gym.ObservationWrapper):
    """
    Transforms the discrete state space into continous.
    For state i will return one-hot vector with index i
    """
    def __init__(self, env):
        super(OneHotWrapper, self).__init__(env)
        self.num_states = env.observation_space.n
        self.observation_space = gym.spaces.Box(0.0, 1.0, (self.num_states))

    def _observation(self, observation):
        one_hot_obs = np.zeros(self.num_states)
        one_hot_obs[observation] = 1.0
        return one_hot_obs



class BufferedObsEnv(gym.ObservationWrapper):
    """Buffer observations and stack e.g. for frame skipping.
    n is the length of the buffer, and number of observations stacked.
    skip is the number of steps between buffered observations (min=1).
    n.b. first obs is the oldest, last obs is the newest.
         the buffer is zeroed out on reset.
         *must* call reset() for init!
    """
    def __init__(self, env=None, n=4, skip=4, shape=(84, 84),
                 channel_last=True):
        super(BufferedObsEnv, self).__init__(env)
        self.obs_shape = shape
        # most recent raw observations (for max pooling across time steps)
        self.obs_buffer = deque(maxlen=2)
        self.n = n
        self.skip = skip
        self.buffer = deque(maxlen=self.n)
        self.counter = 0  # init and reset should agree on this
        shape = shape + (n,) if channel_last else (n,) + shape
        self.observation_space = gym.spaces.Box(0.0, 255.0, shape)
        self.ch_axis = -1 if channel_last else 0
        self.scale = 1.0 / 255
        self.observation_space.high[...] = 1.0

    def _step(self, action):
        obs, reward, done, info = self.env.step(action)
        return self._observation(obs), reward, done, info

    def _observation(self, obs):
        obs = self._convert(obs)
        self.counter += 1
        if self.counter % self.skip == 0:
            self.buffer.append(obs)
        obs_new = np.stack(self.buffer, axis=self.ch_axis)
        return obs_new.astype(np.float32) * self.scale

    def _reset(self):
        """Clear buffer and re-fill by duplicating the first observation."""
        self.obs_buffer.clear()
        obs = self._convert(self.env.reset())
        self.buffer.clear()
        self.counter = 0
        for _ in range(self.n - 1):
            self.buffer.append(np.zeros_like(obs))
        self.buffer.append(obs)
        obs_new = np.stack(self.buffer, axis=self.ch_axis)
        return obs_new.astype(np.float32) * self.scale

    def _convert(self, obs):
        self.obs_buffer.append(obs)
        max_frame = np.max(np.stack(self.obs_buffer), axis=0)
        intensity_frame = self._rgb2y(max_frame).astype(np.uint8)
        small_frame = np.array(Image.fromarray(intensity_frame).resize(
            self.obs_shape, resample=Image.BILINEAR), dtype=np.uint8)
        return small_frame

    def _rgb2y(self, im):
        """Converts an RGB image to a Y image (as in YUV).
        These coefficients are taken from the torch/image library.
        Beware: these are more critical than you might think, as the
        monochromatic contrast can be surprisingly low.
        """
        if len(im.shape) < 3:
            return im
        return np.sum(im * [0.299, 0.587, 0.114], axis=2)


class NoNegativeRewardEnv(gym.RewardWrapper):
    """Clip reward in negative direction."""
    def __init__(self, env=None, neg_clip=0.0):
        super(NoNegativeRewardEnv, self).__init__(env)
        self.neg_clip = neg_clip

    def _reward(self, reward):
        new_reward = self.neg_clip if reward < self.neg_clip else reward
        return new_reward
